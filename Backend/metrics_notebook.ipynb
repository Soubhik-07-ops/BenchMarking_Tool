{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 243 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrroy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 243 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def measure_latency(model, generator, n_runs=100):\n",
    "    times = []\n",
    "    for _ in range(min(n_runs, len(generator.filenames))):  \n",
    "        img, _ = next(generator)\n",
    "        start = time.time()\n",
    "        model.predict(img, verbose=0)\n",
    "        times.append(time.time() - start)\n",
    "    return np.mean(times) * 1000\n",
    "\n",
    "def measure_memory_usage():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def measure_throughput(model, generator):\n",
    "    num_samples = len(generator.filenames)\n",
    "    start = time.time()\n",
    "    model.predict(generator, verbose=0)\n",
    "    return num_samples / (time.time() - start)\n",
    "\n",
    "def benchmark_model(model_path, dataset_path, accuracy_path):\n",
    "    print(f\"üì¶ Benchmarking model: {model_path} with dataset: {dataset_path}\")\n",
    "    model = load_model(model_path)\n",
    "    input_shape = model.input_shape[1:]\n",
    "    target_size = input_shape[:2]\n",
    "    color_mode = 'rgb' if input_shape[-1] == 3 else 'grayscale'\n",
    "\n",
    "    test_gen = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=target_size,\n",
    "        color_mode=color_mode,\n",
    "        batch_size=1,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    with open(accuracy_path, \"r\") as f:\n",
    "        accuracy = json.load(f).get(\"accuracy\", \"N/A\")\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"latency_ms\": round(measure_latency(model, test_gen), 2),\n",
    "        \"model_size_mb\": round(os.path.getsize(model_path) / (1024 * 1024), 2),\n",
    "        \"memory_usage_mb\": round(measure_memory_usage(), 2),\n",
    "        \"throughput_ips\": round(measure_throughput(model, test_gen), 2)\n",
    "    }\n",
    "    print(f\"‚úÖ Metrics for {model_path}: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "def save_metrics(metrics, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "def visualize_comparison(metrics1, metrics2, labels, out_path):\n",
    "    keys = list(metrics1.keys())\n",
    "    vals1 = [float(metrics1[k]) for k in keys]\n",
    "    vals2 = [float(metrics2[k]) for k in keys]\n",
    "\n",
    "    x = np.arange(len(keys))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bar1 = ax.bar(x - width / 2, vals1, width, label=labels[0])\n",
    "    bar2 = ax.bar(x + width / 2, vals2, width, label=labels[1])\n",
    "\n",
    "    ax.set_ylabel('Metric Values')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([k.replace(\"_\", \" \").title() for k in keys])\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_folder = \"uploads\"\n",
    "    output_path = os.path.join(\"results\", \"metrics_output.json\")\n",
    "    plot_path = os.path.join(upload_folder, \"performance_metrics.png\")\n",
    "\n",
    "    try:\n",
    "        with open(\"uploads/config.json\", \"r\") as f:\n",
    "            config_data = json.load(f)\n",
    "\n",
    "        m1 = config_data[\"models\"][0]\n",
    "        m2 = config_data[\"models\"][1]\n",
    "        d1 = config_data[\"datasets\"][0]\n",
    "        d2 = config_data[\"datasets\"][1]\n",
    "        a1 = os.path.join(upload_folder, \"accuracy1.json\")\n",
    "        a2 = os.path.join(upload_folder, \"accuracy2.json\")\n",
    "\n",
    "        metrics1 = benchmark_model(m1, d1, a1)\n",
    "        metrics2 = benchmark_model(m2, d2, a2)\n",
    "\n",
    "        # ‚úÖ Save individual metrics for /get_results API\n",
    "        save_metrics(metrics1, os.path.join(\"results\", \"accuracy1.json\"))\n",
    "        save_metrics(metrics2, os.path.join(\"results\", \"accuracy2.json\"))\n",
    "\n",
    "        # ‚úÖ Save combined metrics + plot\n",
    "        save_metrics({\"Model 1\": metrics1, \"Model 2\": metrics2}, output_path)\n",
    "        visualize_comparison(metrics1, metrics2, [\"Model 1\", \"Model 2\"], plot_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log = os.path.join(upload_folder, \"error.log\")\n",
    "        with open(error_log, \"w\") as f:\n",
    "            f.write(\"‚ùå Error:\\n\" + str(e))\n",
    "        print(\"‚ùå Error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
